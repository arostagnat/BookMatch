{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movies = pd.read_csv(\"./../data/sample_movies_reviews_clean.csv\")\n",
    "X_books = pd.read_csv(\"./../data/sample_books_reviews_clean.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the lists out of the strings and removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books[\"txt_flatten\"] = X_books[\"txt\"].apply(lambda x: eval(x))\n",
    "X_movies[\"txt_flatten\"] = X_movies[\"txt\"].apply(lambda x: eval(x))\n",
    "X_movies.drop(columns=[\"Unnamed: 0\", \"txt\"], inplace=True)\n",
    "X_books.drop(columns=[\"Unnamed: 0\", \"txt\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books_agg = X_books.groupby(\"item_id\").sum()\n",
    "X_movies_agg = X_movies.groupby(\"item_id\").sum()\n",
    "X_books_agg[\"is_movie\"] = 0\n",
    "X_movies_agg[\"is_movie\"] = 1\n",
    "X_books_agg.reset_index(inplace=True)\n",
    "X_movies_agg.reset_index(inplace=True)\n",
    "X_books_agg.rename({\"item_id\":\"item_id_book\"}, axis='columns', inplace=True)\n",
    "X_movies_agg.rename({\"item_id\":\"item_id_movie\"}, axis='columns',inplace=True)\n",
    "X_reviews = pd.concat([X_movies_agg, X_books_agg])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping the reviews by movies_id and book_id\n",
    "### Adding a \"is_movie\" column to keep track of the category of each item\n",
    "### Resetting index in order to keep the item_id for each book and movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movies.groupby(\"item_id\").count().hist(bins=[*range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books.groupby(\"item_id\").count().hist(bins=[*range(40)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution of reviews lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movies_agg[\"length_txt\"] = X_movies_agg[\"txt_flatten\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movies_agg[\"length_txt\"].hist(bins=[*range(0,250,10)])\n",
    "plt.xlabel(\"length_txt\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"# of words per movie review\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books_agg[\"length_txt\"] = X_books_agg[\"txt_flatten\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books_agg[\"length_txt\"].hist(bins=[*range(0,2000,100)])\n",
    "plt.xlabel(\"length_txt\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"# of words per book review\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reviews[\"length_txt\"].hist(bins=50)\n",
    "plt.xlabel(\"length_txt\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"# of words per review (book + movie)\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list the available Word2Vec models\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO use another word2vec\n",
    "#TODO utiliser BERT --> pas forc√©ment besoin de preprocessing ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open('./../notebook_temp/word2vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(word2vec_transfer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load data (deserialize)\n",
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     unserialized_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list Word2Vec params\n",
    "print(word2vec_transfer.vector_size)\n",
    "print(len(word2vec_transfer.key_to_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old embedding version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the training and test sentences\n",
    "# X_embed = embedding(word2vec_transfer, X_reviews[\"txt_flatten\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the training and test embedded sentences\n",
    "# TODO: maxlen could be increased\n",
    "# X_pad = pad_sequences(X_embed, dtype='float32', padding='post', maxlen=1000, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_embed[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model:\n",
    "                try:\n",
    "                    vectors.append(model[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = vectorize(X_reviews[\"txt_flatten\"], model=word2vec_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, Birch, BisectingKMeans\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# # test with \"all-MiniLM-L6-v2\"\n",
    "# # most powerful model : all-mpnet-base-v2\n",
    "\n",
    "# # Store data (serialize)\n",
    "# with open('./../notebook_temp/bert.pickle', 'wb') as handle:\n",
    "#     pickle.dump(bert, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('./../notebook_temp/bert.pickle', 'rb') as handle:\n",
    "    bert = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the files\n",
    "joined_files = os.path.join(\"./../data/proc_data/proc_book/\", \"*.csv\")\n",
    "  \n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Finally, the files are joined\n",
    "X_books_bert = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movies_bert = pd.read_csv(\"./../data/proc_data/proc_movies/mov_chunk1_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movies_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_books_bert[\"is_movie\"] = 0\n",
    "X_movies_bert[\"is_movie\"] = 1\n",
    "# X_books_bert.reset_index(inplace=True)\n",
    "# X_movies_bert.reset_index(inplace=True)\n",
    "X_books_bert.rename({\"item_id\":\"item_id_book\"}, axis='columns', inplace=True)\n",
    "X_movies_bert.rename({\"item_id\":\"item_id_movie\"}, axis='columns',inplace=True)\n",
    "X_reviews_bert = pd.concat([X_movies_bert, X_books_bert], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = bert.encode(X_reviews_bert[\"txt\"]) # ou doc vectorized ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open('./../notebook_temp/bert_embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(bert_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open('./../notebook_temp/bert_embeddings.pickle', 'rb') as handle:\n",
    "    bert_embeddings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatchKmeans (Old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# km = MiniBatchKMeans(n_clusters=75, batch_size=500).fit(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reviews[\"batch_label\"] = km.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=N_CLUSTERS).fit(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_bert = KMeans(n_clusters=N_CLUSTERS).fit(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reviews[\"Kmeans_label\"] = kmeans.labels_\n",
    "X_reviews_bert[\"Kmeans_label_bert\"] = kmeans_bert.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=100).fit(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_bert = AgglomerativeClustering(n_clusters=1500).fit(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reviews[\"clustering_label\"] = clustering.labels_\n",
    "X_reviews_bert[\"clustering_label_bert\"] = clustering_bert.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(vectorized_docs, clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(bert_embeddings, clustering_bert.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage=['ward','complete', \"average\", \"single\"]\n",
    "n_clusters=[50, 75, 100, 125, 150]\n",
    "metric=[\"euclidian\", \"l1\", \"l2\", \"manhattan\", \"cosine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_bert = AgglomerativeClustering(n_clusters=3000, metric=\"cosine\", linkage=\"complete\").fit(bert_embeddings)\n",
    "silhouette_score(bert_embeddings, clustering_bert.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti_k = {}\n",
    "\n",
    "for cluster in [50,100,200]:\n",
    "    opti_k[f\"clustering_bert_{cluster}\"] = AgglomerativeClustering(n_clusters=cluster).fit(bert_embeddings)   \n",
    "    opti_k[f\"silhouette_score_{cluster}\"] = silhouette_score(bert_embeddings, opti_k[f\"clustering_bert_{cluster}\"].labels_)\n",
    "    \n",
    "opti_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti_k2 = {}\n",
    "\n",
    "for cluster in [1500, 2000, 2500, 3000]:\n",
    "    opti_k2[f\"clustering_bert_{cluster}\"] = AgglomerativeClustering(n_clusters=cluster).fit(bert_embeddings)   \n",
    "    opti_k2[f\"silhouette_score_{cluster}\"] = silhouette_score(bert_embeddings, opti_k2[f\"clustering_bert_{cluster}\"].labels_)\n",
    "    \n",
    "opti_k2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BisectingKMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bisect_means = BisectingKMeans(n_clusters=N_CLUSTERS, random_state=1).fit(vectorized_docs)\n",
    "bisect_means_bert = BisectingKMeans(n_clusters=N_CLUSTERS, random_state=1).fit(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reviews[\"bisectings\"] = bisect_means.labels_\n",
    "X_reviews_bert[\"bisectings_bert\"] = bisect_means_bert.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading metadata and merging it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_movies = pd.read_json(\"./../data/raw_data/raw_movies/metadata.json\", lines=True)\n",
    "metadata_books = pd.read_json(\"./../data/raw_data/raw_book/metadata.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_movies.rename({\"item_id\":\"item_id_movie\", \"title\":\"movie_title\"}, axis='columns',inplace=True)\n",
    "metadata_books.rename({\"item_id\":\"item_id_book\", \"title\":\"book_title\"}, axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_movies = pd.merge(X_reviews, metadata_movies, on=\"item_id_movie\", how=\"left\")\n",
    "merged_all = pd.merge(merged_movies, metadata_books, on=\"item_id_book\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_movies_bert = pd.merge(X_reviews_bert, metadata_movies, on=\"item_id_movie\", how=\"left\")\n",
    "merged_all_bert = pd.merge(merged_movies_bert, metadata_books, on=\"item_id_book\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merged_all[[\"movie_title\",\"book_title\",\"clustering_label\", \"Kmeans_label\", \"bisectings\", \"is_movie\" ]]\n",
    "final_df = final_df.fillna(\"\")\n",
    "final_df[\"clustering_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = merged_all_bert[[\"movie_title\",\"book_title\",\"clustering_label_bert\", \"Kmeans_label_bert\", \"bisectings_bert\", \"is_movie\" ]]\n",
    "bert = bert.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df[\"movie_title\"].str.contains(\"potter\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df[\"book_title\"].str.contains(\"potter\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert[bert[\"movie_title\"].str.contains(\"potter\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert[bert[\"book_title\"].str.contains(\"potter\", case=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BookMatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
