{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_reviews = pd.read_csv(\"../data/proc_data/X_raw_1000_jsonlines.csv\")\n",
    "X_book_description = pd.read_json(\"./../data/raw_data/raw_book/metadata.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description.drop(columns=[\"url\", \"authors\", \"lang\", \"img\", \"year\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaner_light(df, list_stop_words=None, see_evolution=False):\n",
    "    \"\"\"Cleaner_light\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): need a column txt for the process\n",
    "        list_stop_words (list(str), optional): a list for remove a few word. Defaults to None.\n",
    "        see_evolution (bool, optional): print the evolution of the function. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: return the DataFrame processed\n",
    "    \"\"\"\n",
    "    # Initialisation\n",
    "    if see_evolution:\n",
    "        print(\"\\nStart Cleaner_light ... ðŸƒ\\nInitialisation ...\\n\")\n",
    "\n",
    "    # Liste de ponctuation conservÃ©e -> !&'(),-.:=?`\n",
    "    # Liste de ponctuation supprimÃ©e -> \"#$%*+/;<>@[\\]^_`{|}~\n",
    "    punctuation = string.punctuation + \"Â–Â—Â•ÂŠ\"\n",
    "    punctuation.replace(\"!&'(),-.:=?`\", \"\")\n",
    "\n",
    "    # Transformation des stop words, copie et ajout d'une majuscule :\n",
    "    # film --> film, Film\n",
    "    if list_stop_words:\n",
    "        list_stop_words_process = []\n",
    "        for word in list_stop_words:\n",
    "            list_stop_words_process.append(word)\n",
    "            list_stop_words_process.append(word.capitalize())\n",
    "\n",
    "    # Run cleaner\n",
    "    if see_evolution:\n",
    "        print(\"Run process ...\")\n",
    "\n",
    "    df.replace({r\"[^\\x00-\\x7F]+\":\"\"}, regex=True, inplace=True)\n",
    "    df.replace(punctuation, \"\")\n",
    "    df.replace(\"\\n\", \"\")\n",
    "\n",
    "\n",
    "    if list_stop_words:\n",
    "        df.txt = [word_tokenize(text) for text in df.txt]\n",
    "        out_list = []\n",
    "        for text in df.txt:\n",
    "            out_text = []\n",
    "            for word in text:\n",
    "                if not word in list_stop_words_process:\n",
    "                    out_text.append(word)\n",
    "            out_list.append(\" \".join(out_text))\n",
    "        df.txt = out_list\n",
    "\n",
    "    if see_evolution:\n",
    "        print(\"\\nâœ… Cleaner_light is done !\\n\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description.rename(columns={\"description\": \"txt\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description.dropna(subset=['txt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description_cleaned = Cleaner_light(X_book_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9309 entries, 0 to 9373\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   item_id  9309 non-null   int64 \n",
      " 1   title    9309 non-null   object\n",
      " 2   txt      9309 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 548.9+ KB\n"
     ]
    }
   ],
   "source": [
    "X_book_description_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>    9309\n",
       "Name: txt, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_book_description_cleaned[\"txt\"].apply(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arostagnat/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-13 12:07:27.935276: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 12:07:28.615129: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-13 12:07:29.802939: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 12:07:29.803150: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 12:07:29.803162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, Birch, BisectingKMeans\n",
    "from sentence_transformers import SentenceTransformer # Make sure you have done a \"pip install -e .\" to have SentenceTransformer package installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4901",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4901",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bert_embeddings \u001b[39m=\u001b[39m bert\u001b[39m.\u001b[39;49mencode(X_book_description_cleaned[\u001b[39m\"\u001b[39;49m\u001b[39mtxt\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \n\u001b[1;32m      2\u001b[0m np\u001b[39m.\u001b[39mshape(bert_embeddings)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:157\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m length_sorted_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort([\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_length(sen) \u001b[39mfor\u001b[39;00m sen \u001b[39min\u001b[39;00m sentences])\n\u001b[0;32m--> 157\u001b[0m sentences_sorted \u001b[39m=\u001b[39m [sentences[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/BookMatch/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 4901"
     ]
    }
   ],
   "source": [
    "bert_embeddings = bert.encode(X_book_description_cleaned[\"txt\"]) \n",
    "np.shape(bert_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTER = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_bert = AgglomerativeClustering(n_clusters=N_CLUSTER).fit(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reviews[\"clustering_label_bert\"] = clustering_bert.labels_\n",
    "X_reviews[\"vector\"] = bert_embeddings # TODO doesn't work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_movies = pd.read_json(\"./../data/raw_data/raw_movies/metadata.json\", lines=True)\n",
    "metadata_books = pd.read_json(\"./../data/raw_data/raw_book/metadata.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_movies.rename({\"item_id\":\"item_id_movie\", \"title\":\"movie_title\"}, axis='columns',inplace=True)\n",
    "metadata_books.rename({\"item_id\":\"item_id_book\", \"title\":\"book_title\"}, axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_movies_complete = pd.merge(X_reviews, metadata_movies, on=\"item_id_movie\", how=\"left\")\n",
    "merged_all_bert = pd.merge(merged_movies_complete, metadata_books, on=\"item_id_book\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = merged_all_bert[[\"movie_title\",\"book_title\",\"clustering_label_bert\",\"is_movie\", \"item_id_movie\", \"item_id_book\" ]]\n",
    "clustered = clustered.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BookMatch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
