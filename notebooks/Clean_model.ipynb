{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_reviews = pd.read_csv(\"../data/proc_data/X_raw_1000_jsonlines.csv\")\n",
    "X_book_description = pd.read_json(\"./../data/raw_data/raw_book/metadata.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movie_description = pd.read_csv(\"./../data/raw_data/raw_movies/df_overview.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movie_description.drop(columns=\"imdb_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movie_description.rename(columns={\"overview\": \"txt\", \"id\":\"item_id\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_movie_description.dropna(subset=['txt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description.drop(columns=[\"url\", \"authors\", \"lang\", \"img\", \"year\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaner_light(df, list_stop_words=None, see_evolution=False):\n",
    "    \"\"\"Cleaner_light\n",
    "    Args:\n",
    "        df (pd.DataFrame): need a column txt for the process\n",
    "        list_stop_words (list(str), optional): a list for remove a few word. Defaults to None.\n",
    "        see_evolution (bool, optional): print the evolution of the function. Defaults to False.\n",
    "    Returns:\n",
    "        pd.DataFrame: return the DataFrame processed\n",
    "    \"\"\"\n",
    "    # Initialisation\n",
    "    if see_evolution:\n",
    "        print(\"\\nStart Cleaner_light ... 🏃\\nInitialisation ...\\n\")\n",
    "\n",
    "    # Liste de ponctuation conservée -> !&'(),-.:=?`\n",
    "    # Liste de ponctuation supprimée -> \"#$%*+/;<>@[\\]^_`{|}~\n",
    "    punctuation = string.punctuation + \"\"\n",
    "    punctuation.replace(\"!&'(),-.:=?`\", \"\")\n",
    "\n",
    "    # Transformation des stop words, copie et ajout d'une majuscule :\n",
    "    # film --> film, Film\n",
    "    if list_stop_words:\n",
    "        list_stop_words_process = []\n",
    "        for word in list_stop_words:\n",
    "            list_stop_words_process.append(word)\n",
    "            list_stop_words_process.append(word.capitalize())\n",
    "\n",
    "    # Run cleaner\n",
    "    if see_evolution:\n",
    "        print(\"Run process ...\")\n",
    "\n",
    "    df.replace({r\"[^\\x00-\\x7F]+\":\"\"}, regex=True, inplace=True)\n",
    "    df.replace(punctuation, \"\")\n",
    "\n",
    "    \n",
    "    if list_stop_words:\n",
    "        df.txt = [word_tokenize(text) for text in df.txt]\n",
    "        out_list = []\n",
    "        for text in df.txt:\n",
    "            out_text = []\n",
    "            for word in text:\n",
    "                if not word in list_stop_words_process:\n",
    "                    out_text.append(word)\n",
    "            out_list.append(\" \".join(out_text))\n",
    "        df.txt = out_list\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    if see_evolution:\n",
    "        print(\"\\n✅ Cleaner_light is done !\\n\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_txt(data,id=\"item_id\",colname=\"txt\"):\n",
    "    \"\"\"\n",
    "    permet de concat les \"txt\" de dataframe par \"item_im\"\n",
    "    et renvoit un df avec autant de lignes que de item_id\n",
    "    \"\"\"\n",
    "    return data.groupby(id, as_index=False).agg({colname: \" \".join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description.rename(columns={\"description\": \"txt\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_book_description.dropna(subset=['txt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_flat=flatten_txt(data=X_book_description,id=\"item_id\",colname=\"txt\")\n",
    "chunk_flat_clean=Cleaner_light(chunk_flat, see_evolution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_flat_movie=flatten_txt(data=X_movie_description,id=\"item_id\",colname=\"txt\")\n",
    "movie_description_cleaned=Cleaner_light(chunk_flat_movie, see_evolution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_description_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_flat_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_description_cleaned[\"is_movie\"] = 1\n",
    "chunk_flat_clean[\"is_movie\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_description = pd.concat([chunk_flat_clean, movie_description_cleaned])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, Birch, BisectingKMeans\n",
    "from sentence_transformers import SentenceTransformer # Make sure you have done a \"pip install -e .\" to have SentenceTransformer package installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = bert.encode(movie_description_cleaned[\"txt\"]) \n",
    "np.shape(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings_books = bert.encode(chunk_flat_clean[\"txt\"]) \n",
    "np.shape(bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_complete = np.vstack((bert_embeddings_books,bert_embeddings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTER = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_bert = AgglomerativeClustering(n_clusters=N_CLUSTER).fit(bert_embedding_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_description[\"clustering_label_bert\"] = clustering_bert.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_movies = pd.read_json(\"./../data/raw_data/raw_movies/metadata.json\", lines=True)\n",
    "metadata_books = pd.read_json(\"./../data/raw_data/raw_book/metadata.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_movies.rename({\"item_id\":\"item_id_movie\", \"title\":\"movie_title\"}, axis='columns',inplace=True)\n",
    "metadata_books.rename({\"item_id\":\"item_id_book\", \"title\":\"book_title\"}, axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_movies_complete = pd.merge(X_reviews, metadata_movies, on=\"item_id_movie\", how=\"left\")\n",
    "merged_all_bert = pd.merge(merged_movies_complete, metadata_books, on=\"item_id_book\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = merged_all_bert[[\"movie_title\",\"book_title\",\"clustering_label_bert\",\"is_movie\", \"item_id_movie\", \"item_id_book\" ]]\n",
    "clustered = clustered.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BookMatch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
