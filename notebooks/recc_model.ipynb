{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load cleaned data\n",
    "X_reviews = pd.read_csv(\"/Users/egmac/code/arostagnat/BookMatch/data/proc_data/cluster_result/X_bert_cluster_150.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load metadata\n",
    "metadata_movies = pd.read_json(\"/Users/egmac/code/arostagnat/BookMatch/data/raw_data/raw_movies/metadata.json\", lines=True)\n",
    "metadata_books = pd.read_json(\"/Users/egmac/code/arostagnat/BookMatch/data/raw_data/raw_book/metadata.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust metadata columns to match X_reviews\n",
    "metadata_movies.rename({\"item_id\":\"item_id_movie\", \"title\":\"title_movie\"}, axis='columns',inplace=True)\n",
    "metadata_books.rename({\"item_id\":\"item_id_book\", \"title\":\"title_book\",\"img\":\"img_book\",\"url\":\"url_book\"}, axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjust import to replace $$$ with 0, and convert item_id to float to enable merge. \n",
    "## Note that the X_reviews import is preformatted as a float\n",
    "# X_reviews = X_reviews.replace({'$$$': 0}, regex=False)\n",
    "X_reviews.item_id_movie = X_reviews.item_id_movie.astype(float)\n",
    "X_reviews.item_id_book = X_reviews.item_id_book.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revise metadata item_id to float to match X_reviews\n",
    "metadata_movies.item_id_movie = metadata_movies.item_id_movie.astype(float)\n",
    "metadata_books.item_id_book = metadata_books.item_id_book.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge X_reviews and metadata\n",
    "X_all = pd.merge(X_reviews, metadata_movies[[\"title_movie\",\"item_id_movie\"]], on=\"item_id_movie\", how=\"left\")\n",
    "X_all = pd.merge(X_all, metadata_books[[\"title_book\",\"item_id_book\",\"url_book\",\"img_book\"]], on=\"item_id_book\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id_movie</th>\n",
       "      <th>is_movie</th>\n",
       "      <th>item_id_book</th>\n",
       "      <th>clustering_label_bert</th>\n",
       "      <th>vector</th>\n",
       "      <th>title_movie</th>\n",
       "      <th>title_book</th>\n",
       "      <th>url_book</th>\n",
       "      <th>img_book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132692.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.79571323e-02  3.01178787e-02 -2.63748504e-...</td>\n",
       "      <td>Frontier Rangers (1959)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id_movie  is_movie  item_id_book  clustering_label_bert  \\\n",
       "0       132692.0       1.0          -1.0                      0   \n",
       "\n",
       "                                              vector              title_movie  \\\n",
       "0  [-1.79571323e-02  3.01178787e-02 -2.63748504e-...  Frontier Rangers (1959)   \n",
       "\n",
       "  title_book url_book img_book  \n",
       "0        NaN      NaN      NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check import\n",
    "X_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27532, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import book ratings for recommendation filtering\n",
    "ratings_books =  pd.read_json(\"/Users/egmac/code/arostagnat/BookMatch/data/raw_data/raw_book/ratings.json\", lines=True)\n",
    "ratings_books = ratings_books.rename(columns={\"item_id\":\"item_id_book\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ratings df: (5152656, 3) | Average ratings df: (9374, 1)\n"
     ]
    }
   ],
   "source": [
    "## Calculate average rating for each book\n",
    "avg_ratings_books = ratings_books.groupby([\"item_id_book\"]).mean().drop(columns=[\"user_id\"])\n",
    "print(f\"Original ratings df: {ratings_books.shape} | Average ratings df: {avg_ratings_books.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant packages\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create extract of X_all with vectors spread across columns, and confirm relevance of dimensions\n",
    "## Vectors have to be reformatted as lists, as they are formatted as strings with \"\"\n",
    "\n",
    "vectors = X_all.vector.tolist()\n",
    "vectors_revised = []\n",
    "\n",
    "for vector in vectors:\n",
    "    result = vector.strip('[]').replace(\"'\",\"\").replace(\"\\n\",\"\").split()\n",
    "    result = [float(i) for i in result]\n",
    "    vectors_revised.append(result)\n",
    "\n",
    "X_vectors = pd.DataFrame(vectors_revised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=X_vectors.shape[1])\n",
    "# svd_result = svd.fit_transform(X_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot variance as a function of the number of components.\n",
    "## Based on the below figure, nearly 100% of the variance is explained by 250 components\n",
    "# plt.plot(svd.explained_variance_ratio_.cumsum())\n",
    "# plt.xlabel('Number of singular value components')\n",
    "# plt.ylabel('Cumulative percent of variance')   \n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape vectors to 250 components, which will help reduce computational time\n",
    "# n = 250\n",
    "# X_vectors_revised = pd.DataFrame(X_vectors.iloc[:,0:n])\n",
    "# print(f'X_all shape: {X_all.shape} | X_vectors shape: {X_vectors.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add qualitative columns to X_vectors_revised, and then create 2 separate dataframes for books and movies.\n",
    "## Movie dataframe to be used to lookup user-inputted movies. Book dataframe to be used for calculations.\n",
    "## Note that the dataframes need to be separated eventually, so it's worth doing now.\n",
    "\n",
    "X_vectors[[\"item_id_movie\",\"item_id_book\",\"is_movie\",\"clustering_label_bert\"]] = X_all[[\"item_id_movie\",\"item_id_book\",\"is_movie\",\"clustering_label_bert\"]]\n",
    "X_vectors_movies = X_vectors[X_vectors.is_movie == 1].set_index(\"item_id_movie\",drop=True).drop(columns=[\"item_id_book\",\"is_movie\"])\n",
    "X_vectors_books = X_vectors[X_vectors.is_movie == 0].set_index(\"item_id_book\",drop=True).drop(columns=[\"item_id_movie\",\"is_movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/hnby49996jgf113yx9b06pd80000gn/T/ipykernel_32289/3642167841.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_vectors_good_books = pd.merge(X_vectors_books,avg_ratings_books,how=\"left\",on=\"item_id_book\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>clustering_label_bert</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id_book</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45524554.0</th>\n",
       "      <td>-0.047604</td>\n",
       "      <td>0.034708</td>\n",
       "      <td>-0.042140</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.029068</td>\n",
       "      <td>-0.003054</td>\n",
       "      <td>0.122946</td>\n",
       "      <td>0.034663</td>\n",
       "      <td>0.016583</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076962</td>\n",
       "      <td>0.074651</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>-0.015889</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>-0.015802</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>-0.044022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44707128.0</th>\n",
       "      <td>-0.042919</td>\n",
       "      <td>-0.050687</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>-0.017286</td>\n",
       "      <td>-0.005120</td>\n",
       "      <td>0.042632</td>\n",
       "      <td>0.030215</td>\n",
       "      <td>-0.024918</td>\n",
       "      <td>0.116753</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010735</td>\n",
       "      <td>-0.023382</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>0.049266</td>\n",
       "      <td>-0.036537</td>\n",
       "      <td>-0.039882</td>\n",
       "      <td>-0.009105</td>\n",
       "      <td>-0.108453</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1         2         3         4         5  \\\n",
       "item_id_book                                                               \n",
       "45524554.0   -0.047604  0.034708 -0.042140  0.023047  0.029068 -0.003054   \n",
       "44707128.0   -0.042919 -0.050687  0.070148 -0.017286 -0.005120  0.042632   \n",
       "\n",
       "                     6         7         8         9  ...       375       376  \\\n",
       "item_id_book                                          ...                       \n",
       "45524554.0    0.122946  0.034663  0.016583  0.012794  ... -0.076962  0.074651   \n",
       "44707128.0    0.030215 -0.024918  0.116753 -0.010577  ... -0.010735 -0.023382   \n",
       "\n",
       "                   377       378       379       380       381       382  \\\n",
       "item_id_book                                                               \n",
       "45524554.0    0.008275 -0.015889  0.030595  0.049729 -0.015802  0.001648   \n",
       "44707128.0    0.011517  0.038426  0.049266 -0.036537 -0.039882 -0.009105   \n",
       "\n",
       "                   383  clustering_label_bert  \n",
       "item_id_book                                   \n",
       "45524554.0   -0.044022                      1  \n",
       "44707128.0   -0.108453                      2  \n",
       "\n",
       "[2 rows x 385 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vectors_good_books = pd.merge(X_vectors_books,avg_ratings_books,how=\"left\",on=\"item_id_book\")\n",
    "X_vectors_good_books = X_vectors_good_books[X_vectors_good_books.rating >= 4].drop(columns=[\"rating\"])\n",
    "X_vectors_good_books.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_vectors_books: (5076, 385) | X_vectors_books.ratings:(2149, 385)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_vectors_books: {X_vectors_books.shape} | X_vectors_books.ratings:{X_vectors_good_books.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: For each film in user list, calculate the cosine similarity with all books in the cluster\n",
    "## Then, sort the books by their cosine similarity to identify **the** **closest** book for each film\n",
    "## Finally, take the full list of book recommendations and then identify the **top 5 most frequent** books\n",
    "\n",
    "def get_local_reccs(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books. Clusters are not used for time being\n",
    "        ### movie_cluster = X_vectors_movies[X_vectors_movies.index == movie_id].cluster_bert.values[0]\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 2: Calculate the average vector for all films in the user list, and then identify the corresponding cluster\n",
    "## Then,calculate the cosine similarity with all books in the cluster\n",
    "## Finally, sort the books by their cosine similarities and take the **top 5 closest** books\n",
    "\n",
    "def get_global_reccs(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\").drop(columns=[\"clustering_label_bert\"])\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)])\n",
    "    books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_rating (user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books. Clusters are not used for time being\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_good_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_rating (user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\").drop(columns=[\"clustering_label_bert\"])\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)])\n",
    "    books_vectors = X_vectors_good_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_cluster(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books.\n",
    "        movie_cluster = X_vectors_movies[X_vectors_movies.index == movie_id].clustering_label_bert.values[0]\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_books[X_vectors_books.clustering_label_bert == movie_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_cluster(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\")\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "    all_movies_vectors = X_vectors_movies.drop(columns=[\"clustering_label_bert\"])\n",
    "    \n",
    "    ## Find cluster of nearest item (film)\n",
    "    sim_movies = cosine_similarity(all_movies_vectors,avg_movie_vector)\n",
    "    sim_movies_detail = pd.DataFrame(sim_movies,\n",
    "                                     index=all_movies_vectors.index,\n",
    "                                     columns=[\"similarity\"]).sort_values(\"similarity\",ascending=False).reset_index()\n",
    "    closest_movie_id = sim_movies_detail.loc[0].item_id_movie\n",
    "    closest_cluster = X_vectors_movies[X_vectors_movies.index == closest_movie_id].clustering_label_bert.values[0]\n",
    "    books_vectors = X_vectors_books[X_vectors_books.clustering_label_bert== closest_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with clusters and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_cluster_rating(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books.\n",
    "        movie_cluster = X_vectors_movies[X_vectors_movies.index == movie_id].clustering_label_bert.values[0]\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_good_books[X_vectors_books.clustering_label_bert == movie_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_cluster_rating(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\")\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "    all_movies_vectors = X_vectors_movies.drop(columns=[\"clustering_label_bert\"])\n",
    "    \n",
    "    ## Find cluster of nearest item (film)\n",
    "    sim_movies = cosine_similarity(all_movies_vectors,avg_movie_vector)\n",
    "    sim_movies_detail = pd.DataFrame(sim_movies,\n",
    "                                     index=all_movies_vectors.index,\n",
    "                                     columns=[\"similarity\"]).sort_values(\"similarity\",ascending=False).reset_index()\n",
    "    closest_movie_id = sim_movies_detail.loc[0].item_id_movie\n",
    "    closest_cluster = X_vectors_movies[X_vectors_movies.index == closest_movie_id].clustering_label_bert.values[0]\n",
    "    books_vectors = X_vectors_good_books[X_vectors_good_books.clustering_label_bert== closest_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = [\"Harry Potter and the Goblet of Fire (2005)\",\n",
    "         \"Maze Runner, The (2014)\",\n",
    "         \"Harry Potter and the Deathly Hallows: Part 2 (2011)\"]\n",
    "\n",
    "suggested_film_ids = []\n",
    "\n",
    "for film in films:\n",
    "    if film in X_all.title_movie.tolist():\n",
    "        movie_id = X_all[X_all.title_movie == film].item_id_movie.values[0]\n",
    "        suggested_film_ids.append(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40815.0, 114180.0, 88125.0]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_film_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "   item_id_movie                                        title_movie\n",
      "0        40815.0         Harry Potter and the Goblet of Fire (2005)\n",
      "1       114180.0                            Maze Runner, The (2014)\n",
      "2        88125.0  Harry Potter and the Deathly Hallows: Part 2 (...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "1              House of Secrets (House of Secrets, #1)\n",
       "2               Frost Like Night (Snow Like Ashes, #3)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "   item_id_movie                                        title_movie\n",
      "0        40815.0         Harry Potter and the Goblet of Fire (2005)\n",
      "1       114180.0                            Maze Runner, The (2014)\n",
      "2        88125.0  Harry Potter and the Deathly Hallows: Part 2 (...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "1                   The Short Drop (Gibson Vaughn, #1)\n",
       "2    Harry Potter and the Sorcerer's Stone (Harry P...\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs_rating(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "   item_id_movie                                        title_movie\n",
      "0        40815.0         Harry Potter and the Goblet of Fire (2005)\n",
      "1       114180.0                            Maze Runner, The (2014)\n",
      "2        88125.0  Harry Potter and the Deathly Hallows: Part 2 (...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "1              House of Secrets (House of Secrets, #1)\n",
       "2    Harry Potter and the Sorcerer's Stone (Harry P...\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs_cluster(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_local_reccs_cluster_rating([1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0           Harry Potter and the Goblet of Fire (2005)\n",
      "1                              Maze Runner, The (2014)\n",
      "2    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "Name: title_movie, dtype: object\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Fantastic Beasts and Where to Find Them: The O...\n",
       "1    Harry Potter and the Cursed Child - Parts One ...\n",
       "2    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "3              The Magician's Land (The Magicians, #3)\n",
       "4               Frost Like Night (Snow Like Ashes, #3)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0           Harry Potter and the Goblet of Fire (2005)\n",
      "1                              Maze Runner, The (2014)\n",
      "2    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "Name: title_movie, dtype: object\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Fantastic Beasts and Where to Find Them: The O...\n",
       "1    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "2              The Magician's Land (The Magicians, #3)\n",
       "3    Harry Potter and the Deathly Hallows (Harry Po...\n",
       "4                  Blood Rites (The Dresden Files, #6)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs_rating(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0           Harry Potter and the Goblet of Fire (2005)\n",
      "1                              Maze Runner, The (2014)\n",
      "2    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "Name: title_movie, dtype: object\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Fantastic Beasts and Where to Find Them: The O...\n",
       "1    Harry Potter and the Cursed Child - Parts One ...\n",
       "2    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "3              The Magician's Land (The Magicians, #3)\n",
       "4              House of Secrets (House of Secrets, #1)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs_cluster(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0           Harry Potter and the Goblet of Fire (2005)\n",
      "1                              Maze Runner, The (2014)\n",
      "2    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "Name: title_movie, dtype: object\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Fantastic Beasts and Where to Find Them: The O...\n",
       "1    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "2              The Magician's Land (The Magicians, #3)\n",
       "3    Harry Potter and the Deathly Hallows (Harry Po...\n",
       "4    Harry Potter and the Sorcerer's Stone (Harry P...\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs_cluster_rating(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of good book recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_movies(user_books:list):\n",
    "    \n",
    "    verified_books = [book_id for book_id in user_books if book_id in X_all.item_id_book.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_movie\"])\n",
    "    books = pd.DataFrame(verified_books,columns=[\"item_id_book\"])\n",
    "    books = pd.merge(books,X_all[[\"title_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    for book_id in verified_books:\n",
    "\n",
    "        # Obtain vectors for user-inputted book\n",
    "        book_cluster = X_vectors_books[X_vectors_books.index == book_id].clustering_label_bert.values[0]\n",
    "        book_vector = X_vectors_books[X_vectors_books.index == book_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        movies_vectors = X_vectors_movies[X_vectors_movies.clustering_label_bert == book_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_movies = cosine_similarity(movies_vectors,book_vector)\n",
    " \n",
    "        # Create summary table of movie with their similarity and relevant details\n",
    "        sim_movies_detail = pd.DataFrame(sim_movies,index=movies_vectors.index,columns=[\"similarity\"])\n",
    "        sim_movies_detail = sim_movies_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_movies_detail = pd.merge(sim_movies_detail,X_all[[\"title_movie\",\"item_id_movie\"]],\n",
    "                                    on=\"item_id_movie\",how=\"left\")\n",
    "\n",
    "        # Add top movie to recommendations dataframe\n",
    "        top_movie = pd.DataFrame([sim_movies_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_movie],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(books)\n",
    "    return recommendations[\"title_movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_movies(user_books:list):\n",
    "    \n",
    "    verified_books = [book_id for book_id in user_books if book_id in X_all.item_id_book.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted books and calculate average vector\n",
    "    books_id = pd.DataFrame(verified_books,columns=[\"item_id_book\"])\n",
    "    books_vectors = pd.merge(books_id,\n",
    "                              X_vectors_books,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_book\").set_index(\"item_id_book\")\n",
    "    avg_book_vector = pd.DataFrame([books_vectors.mean(numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "    all_books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "    \n",
    "    ## Find cluster of nearest item (book)\n",
    "    sim_books = cosine_similarity(all_books_vectors,avg_book_vector)\n",
    "    sim_books_detail = pd.DataFrame(sim_books,\n",
    "                                     index=all_books_vectors.index,\n",
    "                                     columns=[\"similarity\"]).sort_values(\"similarity\",ascending=False).reset_index()\n",
    "    closest_book_id = sim_books_detail.loc[0].item_id_book\n",
    "    closest_cluster = X_vectors_books[X_vectors_books.index == closest_book_id].clustering_label_bert.values[0]\n",
    "    movies_vectors = X_vectors_movies[X_vectors_movies.clustering_label_bert== closest_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_movies = cosine_similarity(movies_vectors,avg_book_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_movies_detail = pd.DataFrame(sim_movies,index=movies_vectors.index,columns=[\"similarity\"])\n",
    "    sim_movies_detail = sim_movies_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_movies_detail = pd.merge(sim_movies_detail,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_movies_detail.head(5)\n",
    "    book_titles = pd.merge(books_id,X_all[[\"title_book\",\"item_id_book\"]],how=\"inner\",on=\"item_id_book\")\n",
    "    print(\"Inputted books\")\n",
    "    print(book_titles.title_book)\n",
    "    print (\"Top 5 movie recommendations\")\n",
    "    return recommendations[\"title_movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(X_all[X_all.is_movie==0].title_book).sort_values(by=[\"title_book\"])\n",
    "books = books.title_book.to_list()\n",
    "# print(*books, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry Potter Boxset (Harry Potter, #1-7)',\n",
       " 'Harry Potter and the Chamber of Secrets (Harry Potter, #2)',\n",
       " 'Harry Potter and the Cursed Child - Parts One and Two (Harry Potter, #8)',\n",
       " 'Harry Potter and the Deathly Hallows (Harry Potter, #7)',\n",
       " 'Harry Potter and the Half-Blood Prince (Harry Potter, #6)',\n",
       " 'Harry Potter and the Methods of Rationality',\n",
       " 'Harry Potter and the Order of the Phoenix (Harry Potter, #5)',\n",
       " 'Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)',\n",
       " \"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\",\n",
       " 'Harry Potter: The Prequel (Harry Potter, #0.5)',\n",
       " \"James Potter and the Hall of Elders' Crossing (James Potter, #1)\",\n",
       " 'Short Stories from Hogwarts of Power, Politics and Pesky Poltergeists (Pottermore Presents, #2)']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_potter = [book for book in books if \"Potter\" in book]\n",
    "harry_potter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_books = [\"Treasure Island\",\n",
    "                   \"Moby-Dick or, The Whale\",\n",
    "                   \"Twenty Thousand Leagues Under the Sea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter_books = [\"Fantastic Beasts and Where to Find Them\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_book_ids = []\n",
    "\n",
    "for book in potter_books:\n",
    "    if book in X_all.title_book.tolist():\n",
    "        book_id = X_all[X_all.title_book == book].item_id_book.values[0]\n",
    "        suggested_book_ids.append(book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4195128.0]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_book_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "   item_id_book                               title_book\n",
      "0     4195128.0  Fantastic Beasts and Where to Find Them\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Harry Potter and the Goblet of Fire (2005)\n",
       "Name: title_movie, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs_movies(suggested_book_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted books\n",
      "0    Fantastic Beasts and Where to Find Them\n",
      "Name: title_book, dtype: object\n",
      "Top 5 movie recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0           Harry Potter and the Goblet of Fire (2005)\n",
       "1    Harry Potter and the Deathly Hallows: Part 2 (...\n",
       "2                              Maze Runner, The (2014)\n",
       "3                                  Going Postal (2010)\n",
       "4    Percy Jackson & the Olympians: The Lightning T...\n",
       "Name: title_movie, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs_movies(suggested_book_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
