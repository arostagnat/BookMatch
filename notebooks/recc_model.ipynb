{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load cleaned data\n",
    "X_reviews = pd.read_csv(\"/Users/egmac/code/arostagnat/BookMatch/data/proc_data/cluster_result/X_bert_cluster_150.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load metadata\n",
    "metadata_movies = pd.read_json(\"/Users/egmac/code/arostagnat/BookMatch/data/raw_data/raw_movies/metadata.json\", lines=True)\n",
    "metadata_books = pd.read_json(\"/Users/egmac/code/arostagnat/BookMatch/data/raw_data/raw_book/metadata.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust metadata columns to match X_reviews\n",
    "metadata_movies.rename({\"item_id\":\"item_id_movie\", \"title\":\"title_movie\"}, axis='columns',inplace=True)\n",
    "metadata_books.rename({\"item_id\":\"item_id_book\", \"title\":\"title_book\",\"img\":\"img_book\",\"url\":\"url_book\"}, axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjust import to replace $$$ with 0, and convert item_id to float to enable merge. \n",
    "## Note that the X_reviews import is preformatted as a float\n",
    "# X_reviews = X_reviews.replace({'$$$': 0}, regex=False)\n",
    "X_reviews.item_id_movie = X_reviews.item_id_movie.astype(float)\n",
    "X_reviews.item_id_book = X_reviews.item_id_book.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revise metadata item_id to float to match X_reviews\n",
    "metadata_movies.item_id_movie = metadata_movies.item_id_movie.astype(float)\n",
    "metadata_books.item_id_book = metadata_books.item_id_book.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge X_reviews and metadata\n",
    "X_all = pd.merge(X_reviews, metadata_movies[[\"title_movie\",\"item_id_movie\"]], on=\"item_id_movie\", how=\"left\")\n",
    "X_all = pd.merge(X_all, metadata_books[[\"title_book\",\"item_id_book\",\"url_book\",\"img_book\"]], on=\"item_id_book\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id_movie</th>\n",
       "      <th>is_movie</th>\n",
       "      <th>item_id_book</th>\n",
       "      <th>clustering_label_bert</th>\n",
       "      <th>vector</th>\n",
       "      <th>title_movie</th>\n",
       "      <th>title_book</th>\n",
       "      <th>url_book</th>\n",
       "      <th>img_book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132692.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.79571323e-02  3.01178787e-02 -2.63748504e-...</td>\n",
       "      <td>Frontier Rangers (1959)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id_movie  is_movie  item_id_book  clustering_label_bert  \\\n",
       "0       132692.0       1.0          -1.0                      0   \n",
       "\n",
       "                                              vector              title_movie  \\\n",
       "0  [-1.79571323e-02  3.01178787e-02 -2.63748504e-...  Frontier Rangers (1959)   \n",
       "\n",
       "  title_book url_book img_book  \n",
       "0        NaN      NaN      NaN  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check import\n",
    "X_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27532, 9)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import book ratings for recommendation filtering\n",
    "ratings_books =  pd.read_json(\"/Users/egmac/code/arostagnat/BookMatch/data/raw_data/raw_book/ratings.json\", lines=True)\n",
    "ratings_books = ratings_books.rename(columns={\"item_id\":\"item_id_book\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ratings df: (5152656, 3) | Average ratings df: (9374, 1)\n"
     ]
    }
   ],
   "source": [
    "## Calculate average rating for each book\n",
    "avg_ratings_books = ratings_books.groupby([\"item_id_book\"]).mean().drop(columns=[\"user_id\"])\n",
    "print(f\"Original ratings df: {ratings_books.shape} | Average ratings df: {avg_ratings_books.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant packages\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create extract of X_all with vectors spread across columns, and confirm relevance of dimensions\n",
    "## Vectors have to be reformatted as lists, as they are formatted as strings with \"\"\n",
    "\n",
    "vectors = X_all.vector.tolist()\n",
    "vectors_revised = []\n",
    "\n",
    "for vector in vectors:\n",
    "    result = vector.strip('[]').replace(\"'\",\"\").replace(\"\\n\",\"\").split()\n",
    "    result = [float(i) for i in result]\n",
    "    vectors_revised.append(result)\n",
    "\n",
    "X_vectors = pd.DataFrame(vectors_revised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=X_vectors.shape[1])\n",
    "# svd_result = svd.fit_transform(X_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot variance as a function of the number of components.\n",
    "## Based on the below figure, nearly 100% of the variance is explained by 250 components\n",
    "# plt.plot(svd.explained_variance_ratio_.cumsum())\n",
    "# plt.xlabel('Number of singular value components')\n",
    "# plt.ylabel('Cumulative percent of variance')   \n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape vectors to 250 components, which will help reduce computational time\n",
    "# n = 250\n",
    "# X_vectors_revised = pd.DataFrame(X_vectors.iloc[:,0:n])\n",
    "# print(f'X_all shape: {X_all.shape} | X_vectors shape: {X_vectors.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add qualitative columns to X_vectors_revised, and then create 2 separate dataframes for books and movies.\n",
    "## Movie dataframe to be used to lookup user-inputted movies. Book dataframe to be used for calculations.\n",
    "## Note that the dataframes need to be separated eventually, so it's worth doing now.\n",
    "\n",
    "X_vectors[[\"item_id_movie\",\"item_id_book\",\"is_movie\",\"clustering_label_bert\"]] = X_all[[\"item_id_movie\",\"item_id_book\",\"is_movie\",\"clustering_label_bert\"]]\n",
    "X_vectors_movies = X_vectors[X_vectors.is_movie == 1].set_index(\"item_id_movie\",drop=True).drop(columns=[\"item_id_book\",\"is_movie\"])\n",
    "X_vectors_books = X_vectors[X_vectors.is_movie == 0].set_index(\"item_id_book\",drop=True).drop(columns=[\"item_id_movie\",\"is_movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/hnby49996jgf113yx9b06pd80000gn/T/ipykernel_32289/3642167841.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_vectors_good_books = pd.merge(X_vectors_books,avg_ratings_books,how=\"left\",on=\"item_id_book\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>clustering_label_bert</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id_book</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45524554.0</th>\n",
       "      <td>-0.047604</td>\n",
       "      <td>0.034708</td>\n",
       "      <td>-0.042140</td>\n",
       "      <td>0.023047</td>\n",
       "      <td>0.029068</td>\n",
       "      <td>-0.003054</td>\n",
       "      <td>0.122946</td>\n",
       "      <td>0.034663</td>\n",
       "      <td>0.016583</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076962</td>\n",
       "      <td>0.074651</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>-0.015889</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>-0.015802</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>-0.044022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44707128.0</th>\n",
       "      <td>-0.042919</td>\n",
       "      <td>-0.050687</td>\n",
       "      <td>0.070148</td>\n",
       "      <td>-0.017286</td>\n",
       "      <td>-0.005120</td>\n",
       "      <td>0.042632</td>\n",
       "      <td>0.030215</td>\n",
       "      <td>-0.024918</td>\n",
       "      <td>0.116753</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010735</td>\n",
       "      <td>-0.023382</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>0.038426</td>\n",
       "      <td>0.049266</td>\n",
       "      <td>-0.036537</td>\n",
       "      <td>-0.039882</td>\n",
       "      <td>-0.009105</td>\n",
       "      <td>-0.108453</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1         2         3         4         5  \\\n",
       "item_id_book                                                               \n",
       "45524554.0   -0.047604  0.034708 -0.042140  0.023047  0.029068 -0.003054   \n",
       "44707128.0   -0.042919 -0.050687  0.070148 -0.017286 -0.005120  0.042632   \n",
       "\n",
       "                     6         7         8         9  ...       375       376  \\\n",
       "item_id_book                                          ...                       \n",
       "45524554.0    0.122946  0.034663  0.016583  0.012794  ... -0.076962  0.074651   \n",
       "44707128.0    0.030215 -0.024918  0.116753 -0.010577  ... -0.010735 -0.023382   \n",
       "\n",
       "                   377       378       379       380       381       382  \\\n",
       "item_id_book                                                               \n",
       "45524554.0    0.008275 -0.015889  0.030595  0.049729 -0.015802  0.001648   \n",
       "44707128.0    0.011517  0.038426  0.049266 -0.036537 -0.039882 -0.009105   \n",
       "\n",
       "                   383  clustering_label_bert  \n",
       "item_id_book                                   \n",
       "45524554.0   -0.044022                      1  \n",
       "44707128.0   -0.108453                      2  \n",
       "\n",
       "[2 rows x 385 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vectors_good_books = pd.merge(X_vectors_books,avg_ratings_books,how=\"left\",on=\"item_id_book\")\n",
    "X_vectors_good_books = X_vectors_good_books[X_vectors_good_books.rating >= 4].drop(columns=[\"rating\"])\n",
    "X_vectors_good_books.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_vectors_books: (5076, 385) | X_vectors_books.ratings:(2149, 385)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_vectors_books: {X_vectors_books.shape} | X_vectors_books.ratings:{X_vectors_good_books.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1: For each film in user list, calculate the cosine similarity with all books in the cluster\n",
    "## Then, sort the books by their cosine similarity to identify **the** **closest** book for each film\n",
    "## Finally, take the full list of book recommendations and then identify the **top 5 most frequent** books\n",
    "\n",
    "def get_local_reccs(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books. Clusters are not used for time being\n",
    "        ### movie_cluster = X_vectors_movies[X_vectors_movies.index == movie_id].cluster_bert.values[0]\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 2: Calculate the average vector for all films in the user list, and then identify the corresponding cluster\n",
    "## Then,calculate the cosine similarity with all books in the cluster\n",
    "## Finally, sort the books by their cosine similarities and take the **top 5 closest** books\n",
    "\n",
    "def get_global_reccs(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\").drop(columns=[\"clustering_label_bert\"])\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)])\n",
    "    books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_rating (user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books. Clusters are not used for time being\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_good_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_rating (user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\").drop(columns=[\"clustering_label_bert\"])\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)])\n",
    "    books_vectors = X_vectors_good_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_cluster(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books.\n",
    "        movie_cluster = X_vectors_movies[X_vectors_movies.index == movie_id].clustering_label_bert.values[0]\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_books[X_vectors_books.clustering_label_bert == movie_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_cluster(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\")\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "    all_movies_vectors = X_vectors_movies.drop(columns=[\"clustering_label_bert\"])\n",
    "    \n",
    "    ## Find cluster of nearest item (film)\n",
    "    sim_movies = cosine_similarity(all_movies_vectors,avg_movie_vector)\n",
    "    sim_movies_detail = pd.DataFrame(sim_movies,\n",
    "                                     index=all_movies_vectors.index,\n",
    "                                     columns=[\"similarity\"]).sort_values(\"similarity\",ascending=False).reset_index()\n",
    "    closest_movie_id = sim_movies_detail.loc[0].item_id_movie\n",
    "    closest_cluster = X_vectors_movies[X_vectors_movies.index == closest_movie_id].clustering_label_bert.values[0]\n",
    "    books_vectors = X_vectors_books[X_vectors_books.clustering_label_bert== closest_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.021117</td>\n",
       "      <td>-0.031267</td>\n",
       "      <td>0.077505</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>0.02245</td>\n",
       "      <td>0.071607</td>\n",
       "      <td>0.057103</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.070925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061121</td>\n",
       "      <td>-0.022265</td>\n",
       "      <td>-0.011456</td>\n",
       "      <td>0.040391</td>\n",
       "      <td>-0.025528</td>\n",
       "      <td>-0.032638</td>\n",
       "      <td>0.033434</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.05141</td>\n",
       "      <td>0.039787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4        5         6    \\\n",
       "0 -0.021117 -0.031267  0.077505  0.017031 -0.013605  0.02245  0.071607   \n",
       "\n",
       "        7         8         9    ...       374       375       376       377  \\\n",
       "0  0.057103  0.003765  0.070925  ...  0.061121 -0.022265 -0.011456  0.040391   \n",
       "\n",
       "        378       379       380       381      382       383  \n",
       "0 -0.025528 -0.032638  0.033434  0.007523  0.05141  0.039787  \n",
       "\n",
       "[1 rows x 384 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movies = [1,1,3,4,5,6,7,8]\n",
    "verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "\n",
    "## Collect clusters of all inputted films and identify most frequent cluster\n",
    "movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "movies_vectors = pd.merge(movies_id,\n",
    "                          X_vectors_movies,\n",
    "                          how=\"left\",\n",
    "                          on=\"item_id_movie\").set_index(\"item_id_movie\")\n",
    "movies_clusters = movies_vectors.groupby([\"clustering_label_bert\"]).count()[0].reset_index().rename(columns={\n",
    "    0:\"cluster_count\"})\n",
    "movies_clusters = movies_clusters.sort_values(by=\"cluster_count\",ascending=False).reset_index().drop(columns=[\"index\"])\n",
    "counts = movies_clusters.cluster_count.to_list()\n",
    "avg_cluster = movies_clusters.loc[0][\"clustering_label_bert\"]\n",
    "avg_movie_vector = pd.DataFrame([movies_vectors[movies_vectors.clustering_label_bert == main_cluster].mean(\n",
    "            numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "avg_movie_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_cluster_reccs(user_movies:list):\n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect clusters of all inputted films and identify most frequent cluster\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\")\n",
    "    \n",
    "    movies_clusters = movies_vectors.groupby([\"clustering_label_bert\"]).count()[0].reset_index().rename(columns={\n",
    "    0:\"cluster_count\"})\n",
    "    movies_clusters = movies_clusters.sort_values(by=\"cluster_count\",ascending=False).reset_index().drop(columns=[\"index\"])\n",
    "    movies_clusters_counts = movies_clusters.cluster_count.to_list()\n",
    "    main_cluster = movies_clusters.loc[0][\"clustering_label_bert\"]\n",
    "    \n",
    "    if main_cluster > 1:\n",
    "        \n",
    "        avg_movie_vector = pd.DataFrame([movies_vectors[movies_vectors.clustering_label_bert == main_cluster].mean(\n",
    "            numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "        \n",
    "#         books_vectors = X_vectors_books[X_vectors_books.clustering_label_bert== main_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "        \n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        ## Take top 5 books and show results\n",
    "        recommendations = sim_books_detail.head(5)\n",
    "        movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "        print(\"Inputted films\")\n",
    "        print(movie_titles.title_movie)\n",
    "        print(movies_clusters)\n",
    "        print (\"Top 5 book recommendations\")\n",
    "        return recommendations[\"title_book\"]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "#         all_movies_vectors = X_vectors_movies.drop(columns=[\"clustering_label_bert\"])\n",
    "#         sim_movies = cosine_similarity(all_movies_vectors,avg_movie_vector)\n",
    "#         sim_movies_detail = pd.DataFrame(sim_movies,\n",
    "#                                          index=all_movies_vectors.index,\n",
    "#                                          columns=[\"similarity\"]).sort_values(\"similarity\",ascending=False).reset_index()\n",
    "#         closest_movie_id = sim_movies_detail.loc[0].item_id_movie\n",
    "#         closest_cluster = X_vectors_movies[X_vectors_movies.index == closest_movie_id].clustering_label_bert.values[0]\n",
    "        books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        ## Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "        ## Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        ## Take top 5 books and show results\n",
    "        recommendations = sim_books_detail.head(5)\n",
    "        movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "        print(\"Inputted films\")\n",
    "        print(movie_titles.title_movie)\n",
    "        print(movies_clusters)\n",
    "        print (\"Top 5 book recommendations\")\n",
    "        return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with clusters and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_cluster_rating(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_book\",\"img_book\",\"url_book\"])\n",
    "    movies = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies = pd.merge(movies,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    for movie_id in verified_movies:\n",
    "\n",
    "        # Obtain vectors for user-inputted film and all books.\n",
    "        movie_cluster = X_vectors_movies[X_vectors_movies.index == movie_id].clustering_label_bert.values[0]\n",
    "        movie_vector = X_vectors_movies[X_vectors_movies.index == movie_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        books_vectors = X_vectors_good_books[X_vectors_books.clustering_label_bert == movie_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_books = cosine_similarity(books_vectors,movie_vector)\n",
    " \n",
    "        # Create summary table of books with their similarity and relevant details\n",
    "        sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "        sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],\n",
    "                                    on=\"item_id_book\",how=\"left\")\n",
    "\n",
    "        # Add top book to recommendations dataframe\n",
    "        top_book = pd.DataFrame([sim_books_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_book],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(movies)\n",
    "    return recommendations[\"title_book\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_cluster_rating(user_movies:list):\n",
    "    \n",
    "    verified_movies = [movie_id for movie_id in user_movies if movie_id in X_all.item_id_movie.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted films and calculate average vector\n",
    "    movies_id = pd.DataFrame(verified_movies,columns=[\"item_id_movie\"])\n",
    "    movies_vectors = pd.merge(movies_id,\n",
    "                              X_vectors_movies,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_movie\").set_index(\"item_id_movie\")\n",
    "    avg_movie_vector = pd.DataFrame([movies_vectors.mean(numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "    all_movies_vectors = X_vectors_movies.drop(columns=[\"clustering_label_bert\"])\n",
    "    \n",
    "    ## Find cluster of nearest item (film)\n",
    "    sim_movies = cosine_similarity(all_movies_vectors,avg_movie_vector)\n",
    "    sim_movies_detail = pd.DataFrame(sim_movies,\n",
    "                                     index=all_movies_vectors.index,\n",
    "                                     columns=[\"similarity\"]).sort_values(\"similarity\",ascending=False).reset_index()\n",
    "    closest_movie_id = sim_movies_detail.loc[0].item_id_movie\n",
    "    closest_cluster = X_vectors_movies[X_vectors_movies.index == closest_movie_id].clustering_label_bert.values[0]\n",
    "    books_vectors = X_vectors_good_books[X_vectors_goodb_books.clustering_label_bert== closest_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_books = cosine_similarity(books_vectors,avg_movie_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_books_detail = pd.DataFrame(sim_books,index=books_vectors.index,columns=[\"similarity\"])\n",
    "    sim_books_detail = sim_books_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_books_detail = pd.merge(sim_books_detail,X_all[[\"title_book\",\"img_book\",\"url_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_books_detail.head(5)\n",
    "    movie_titles = pd.merge(movies_id,X_all[[\"title_movie\",\"item_id_movie\"]],how=\"inner\",on=\"item_id_movie\")\n",
    "    print(\"Inputted films\")\n",
    "    print(movie_titles.title_movie)\n",
    "    print (\"Top 5 book recommendations\")\n",
    "    return recommendations[\"title_book\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\",\n",
       " 'Harry Potter and the Goblet of Fire (2005)',\n",
       " 'Private Potter (1962)',\n",
       " 'Harry Potter and the Deathly Hallows: Part 2 (2011)',\n",
       " 'Very Potter Musical, A (2009)']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# films = [\"Harry Potter and the Goblet of Fire (2005)\",\n",
    "#          \"Maze Runner, The (2014)\",\n",
    "#          \"Harry Potter and the Deathly Hallows: Part 2 (2011)\"]\n",
    "\n",
    "films = X_all.title_movie.dropna().to_list()\n",
    "harry_potter_movies = [film for film in films if \"Potter\" in film]\n",
    "harry_potter_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ali Zoua: Prince of the Streets (Ali Zaoua, prince de la rue) (2000)',\n",
       " 'Princess of Montpensier, The (La princesse de Montpensier) (2010)']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_movies = [film for film in films if \"prince\" in film]\n",
    "cluster_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_film_ids = []\n",
    "\n",
    "for film in harry_potter_movies:\n",
    "    if film in X_all.title_movie.tolist():\n",
    "        movie_id = X_all[X_all.title_movie == film].item_id_movie.values[0]\n",
    "        suggested_film_ids.append(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_ids(movie_titles):\n",
    "    movie_ids = []\n",
    "    for movie in movie_titles:\n",
    "        if movie in X_all.title_movie.tolist():\n",
    "            movie_id = X_all[X_all.title_movie == movie].item_id_movie.values[0]\n",
    "            movie_ids.append(movie_id)\n",
    "    return movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_film_ids = get_movie_ids(cluster_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "   item_id_movie                                        title_movie\n",
      "0         4896.0  Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1        40815.0         Harry Potter and the Goblet of Fire (2005)\n",
      "2       150330.0                              Private Potter (1962)\n",
      "3        88125.0  Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "4        93006.0                      Very Potter Musical, A (2009)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                     The Magicians (The Magicians #1)\n",
       "1    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "2                                         Redeployment\n",
       "3               Frost Like Night (Snow Like Ashes, #3)\n",
       "4             The Walking Dead, Vol. 01: Days Gone Bye\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "   item_id_movie                                        title_movie\n",
      "0         4896.0  Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1        40815.0         Harry Potter and the Goblet of Fire (2005)\n",
      "2       150330.0                              Private Potter (1962)\n",
      "3        88125.0  Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "4        93006.0                      Very Potter Musical, A (2009)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Harry Potter and the Order of the Phoenix (Har...\n",
       "1    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "2                                         Redeployment\n",
       "3    Harry Potter and the Sorcerer's Stone (Harry P...\n",
       "4             The Walking Dead, Vol. 01: Days Gone Bye\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs_rating(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "   item_id_movie                                        title_movie\n",
      "0         4896.0  Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1        40815.0         Harry Potter and the Goblet of Fire (2005)\n",
      "2       150330.0                              Private Potter (1962)\n",
      "3        88125.0  Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "4        93006.0                      Very Potter Musical, A (2009)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                           My Man Jeeves (Jeeves, #1)\n",
       "1    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "2                   It's in His Kiss (Bridgertons, #7)\n",
       "3    Harry Potter and the Sorcerer's Stone (Harry P...\n",
       "4                      Rock Hard (Sinners on Tour, #2)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs_cluster(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_local_reccs_cluster_rating([1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0    Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1           Harry Potter and the Goblet of Fire (2005)\n",
      "2                                Private Potter (1962)\n",
      "3    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "4                        Very Potter Musical, A (2009)\n",
      "Name: title_movie, dtype: object\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Fantastic Beasts and Where to Find Them: The O...\n",
       "1    Harry Potter and the Cursed Child - Parts One ...\n",
       "2    Harry Potter and the Order of the Phoenix (Har...\n",
       "3    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "4              The Magician's Land (The Magicians, #3)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0    Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1           Harry Potter and the Goblet of Fire (2005)\n",
      "2                                Private Potter (1962)\n",
      "3    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "4                        Very Potter Musical, A (2009)\n",
      "Name: title_movie, dtype: object\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Fantastic Beasts and Where to Find Them: The O...\n",
       "1    Harry Potter and the Order of the Phoenix (Har...\n",
       "2    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "3              The Magician's Land (The Magicians, #3)\n",
       "4                       Wicked Heart (Starcrossed, #3)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs_rating(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0    Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1           Harry Potter and the Goblet of Fire (2005)\n",
      "2                                Private Potter (1962)\n",
      "3    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "4                        Very Potter Musical, A (2009)\n",
      "Name: title_movie, dtype: object\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Fantastic Beasts and Where to Find Them: The O...\n",
       "1    Harry Potter and the Cursed Child - Parts One ...\n",
       "2    Harry Potter and the Order of the Phoenix (Har...\n",
       "3    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "4              The Magician's Land (The Magicians, #3)\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs_cluster(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_global_reccs_cluster_rating(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0    Harry Potter and the Sorcerer's Stone (a.k.a. ...\n",
      "1           Harry Potter and the Goblet of Fire (2005)\n",
      "2                                Private Potter (1962)\n",
      "3    Harry Potter and the Deathly Hallows: Part 2 (...\n",
      "4                        Very Potter Musical, A (2009)\n",
      "Name: title_movie, dtype: object\n",
      "   clustering_label_bert  cluster_count\n",
      "0                     13              2\n",
      "1                     15              1\n",
      "2                     29              1\n",
      "3                     64              1\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Harry Potter and the Prisoner of Azkaban (Harr...\n",
       "1    Fantastic Beasts and Where to Find Them: The O...\n",
       "2    Harry Potter and the Order of the Phoenix (Har...\n",
       "3    Harry Potter and the Deathly Hallows (Harry Po...\n",
       "4    Harry Potter and the Sorcerer's Stone (Harry P...\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_freq_cluster_reccs(suggested_film_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "0                                St George's Day (2012)\n",
      "1                  10,000 Black Men Named George (2002)\n",
      "2        Curious George: A Very Monkey Christmas (2009)\n",
      "3                           George of the Jungle (1997)\n",
      "4                    Madness of King George, The (1994)\n",
      "5          George Carlin: Playin' with Your Head (1986)\n",
      "6         George Carlin: Complaints & Grievances (2001)\n",
      "7                         George of the Jungle 2 (2003)\n",
      "8     Paul McCartney Really Is Dead: The Last Testam...\n",
      "9                                 Curious George (2006)\n",
      "10                               Come on George! (1939)\n",
      "11              My Letter to George (Mesmerized) (1986)\n",
      "12                          Journeys with George (2002)\n",
      "13                  George Washington Slept Here (1942)\n",
      "14                         Le roman de Georgette (2003)\n",
      "15                 Killing of Sister George, The (1968)\n",
      "16                  People vs. George Lucas, The (2010)\n",
      "Name: title_movie, dtype: object\n",
      "    clustering_label_bert  cluster_count\n",
      "0                      64              2\n",
      "1                     131              2\n",
      "2                       4              1\n",
      "3                      11              1\n",
      "4                      15              1\n",
      "5                      25              1\n",
      "6                      29              1\n",
      "7                      46              1\n",
      "8                      52              1\n",
      "9                      56              1\n",
      "10                     75              1\n",
      "11                     88              1\n",
      "12                     91              1\n",
      "13                    102              1\n",
      "14                    135              1\n",
      "Top 5 book recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    The Girl with a Clock for a Heart\n",
       "1                       The Ice Dragon\n",
       "2         Tinker, Tailor, Soldier, Spy\n",
       "3                      The Book of Joe\n",
       "4                      Beautiful Ruins\n",
       "Name: title_book, dtype: object"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_freq_cluster_reccs(cluster_film_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of good book recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_reccs_movies(user_books:list):\n",
    "    \n",
    "    verified_books = [book_id for book_id in user_books if book_id in X_all.item_id_book.tolist()]\n",
    "    \n",
    "    recommendations = pd.DataFrame(columns=[\"similarity\",\"title_movie\"])\n",
    "    books = pd.DataFrame(verified_books,columns=[\"item_id_book\"])\n",
    "    books = pd.merge(books,X_all[[\"title_book\",\"item_id_book\"]],on=\"item_id_book\",how=\"left\")\n",
    "    \n",
    "    for book_id in verified_books:\n",
    "\n",
    "        # Obtain vectors for user-inputted book\n",
    "        book_cluster = X_vectors_books[X_vectors_books.index == book_id].clustering_label_bert.values[0]\n",
    "        book_vector = X_vectors_books[X_vectors_books.index == book_id].drop(columns=[\"clustering_label_bert\"])\n",
    "        movies_vectors = X_vectors_movies[X_vectors_movies.clustering_label_bert == book_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        sim_movies = cosine_similarity(movies_vectors,book_vector)\n",
    " \n",
    "        # Create summary table of movie with their similarity and relevant details\n",
    "        sim_movies_detail = pd.DataFrame(sim_movies,index=movies_vectors.index,columns=[\"similarity\"])\n",
    "        sim_movies_detail = sim_movies_detail.sort_values(\"similarity\",ascending=False)\n",
    "        sim_movies_detail = pd.merge(sim_movies_detail,X_all[[\"title_movie\",\"item_id_movie\"]],\n",
    "                                    on=\"item_id_movie\",how=\"left\")\n",
    "\n",
    "        # Add top movie to recommendations dataframe\n",
    "        top_movie = pd.DataFrame([sim_movies_detail.loc[0]])\n",
    "        recommendations = pd.concat([recommendations,top_movie],axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Inputted films\")\n",
    "    print(books)\n",
    "    return recommendations[\"title_movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_reccs_movies(user_books:list):\n",
    "    \n",
    "    verified_books = [book_id for book_id in user_books if book_id in X_all.item_id_book.tolist()]\n",
    "    \n",
    "    ## Collect vectors of all inputted books and calculate average vector\n",
    "    books_id = pd.DataFrame(verified_books,columns=[\"item_id_book\"])\n",
    "    books_vectors = pd.merge(books_id,\n",
    "                              X_vectors_books,\n",
    "                              how=\"left\",\n",
    "                              on=\"item_id_book\").set_index(\"item_id_book\")\n",
    "    avg_book_vector = pd.DataFrame([books_vectors.mean(numeric_only=True)]).drop(columns=[\"clustering_label_bert\"])\n",
    "    all_books_vectors = X_vectors_books.drop(columns=[\"clustering_label_bert\"])\n",
    "    \n",
    "    ## Find cluster of nearest item (book)\n",
    "    sim_books = cosine_similarity(all_books_vectors,avg_book_vector)\n",
    "    sim_books_detail = pd.DataFrame(sim_books,\n",
    "                                     index=all_books_vectors.index,\n",
    "                                     columns=[\"similarity\"]).sort_values(\"similarity\",ascending=False).reset_index()\n",
    "    closest_book_id = sim_books_detail.loc[0].item_id_book\n",
    "    closest_cluster = X_vectors_books[X_vectors_books.index == closest_book_id].clustering_label_bert.values[0]\n",
    "    movies_vectors = X_vectors_movies[X_vectors_movies.clustering_label_bert== closest_cluster].drop(columns=[\"clustering_label_bert\"])\n",
    "\n",
    "    ## Calculate cosine similarity\n",
    "    sim_movies = cosine_similarity(movies_vectors,avg_book_vector)\n",
    "\n",
    "    ## Create summary table of books with their similarity and relevant details\n",
    "    sim_movies_detail = pd.DataFrame(sim_movies,index=movies_vectors.index,columns=[\"similarity\"])\n",
    "    sim_movies_detail = sim_movies_detail.sort_values(\"similarity\",ascending=False)\n",
    "    sim_movies_detail = pd.merge(sim_movies_detail,X_all[[\"title_movie\",\"item_id_movie\"]],on=\"item_id_movie\",how=\"left\")\n",
    "    \n",
    "    ## Take top 5 books and show results\n",
    "    recommendations = sim_movies_detail.head(5)\n",
    "    book_titles = pd.merge(books_id,X_all[[\"title_book\",\"item_id_book\"]],how=\"inner\",on=\"item_id_book\")\n",
    "    print(\"Inputted books\")\n",
    "    print(book_titles.title_book)\n",
    "    print (\"Top 5 movie recommendations\")\n",
    "    return recommendations[\"title_movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.DataFrame(X_all[X_all.is_movie==0].title_book).sort_values(by=[\"title_book\"])\n",
    "books = books.title_book.to_list()\n",
    "# print(*books, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry Potter Boxset (Harry Potter, #1-7)',\n",
       " 'Harry Potter and the Chamber of Secrets (Harry Potter, #2)',\n",
       " 'Harry Potter and the Cursed Child - Parts One and Two (Harry Potter, #8)',\n",
       " 'Harry Potter and the Deathly Hallows (Harry Potter, #7)',\n",
       " 'Harry Potter and the Half-Blood Prince (Harry Potter, #6)',\n",
       " 'Harry Potter and the Methods of Rationality',\n",
       " 'Harry Potter and the Order of the Phoenix (Harry Potter, #5)',\n",
       " 'Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)',\n",
       " \"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\",\n",
       " 'Harry Potter: The Prequel (Harry Potter, #0.5)',\n",
       " \"James Potter and the Hall of Elders' Crossing (James Potter, #1)\",\n",
       " 'Short Stories from Hogwarts of Power, Politics and Pesky Poltergeists (Pottermore Presents, #2)']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_potter = [book for book in books if \"Potter\" in book]\n",
    "harry_potter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_books = [book for book in books if \"treasure\" in book]\n",
    "other_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_books = [\"Treasure Island\",\n",
    "                   \"Moby-Dick or, The Whale\",\n",
    "                   \"Twenty Thousand Leagues Under the Sea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "potter_books = [\"Fantastic Beasts and Where to Find Them\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_book_ids = []\n",
    "\n",
    "for book in harry_potter:\n",
    "    if book in X_all.title_book.tolist():\n",
    "        book_id = X_all[X_all.title_book == book].item_id_book.values[0]\n",
    "        suggested_book_ids.append(book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2962492.0,\n",
       " 6231171.0,\n",
       " 48765776.0,\n",
       " 2963218.0,\n",
       " 41335427.0,\n",
       " 14911331.0,\n",
       " 2809203.0,\n",
       " 2402163.0,\n",
       " 4640799.0,\n",
       " 13810543.0,\n",
       " 2556437.0,\n",
       " 52218734.0]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_book_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted films\n",
      "    item_id_book                                         title_book\n",
      "0      2962492.0           Harry Potter Boxset (Harry Potter, #1-7)\n",
      "1      6231171.0  Harry Potter and the Chamber of Secrets (Harry...\n",
      "2     48765776.0  Harry Potter and the Cursed Child - Parts One ...\n",
      "3      2963218.0  Harry Potter and the Deathly Hallows (Harry Po...\n",
      "4     41335427.0  Harry Potter and the Half-Blood Prince (Harry ...\n",
      "5     14911331.0        Harry Potter and the Methods of Rationality\n",
      "6      2809203.0  Harry Potter and the Order of the Phoenix (Har...\n",
      "7      2402163.0  Harry Potter and the Prisoner of Azkaban (Harr...\n",
      "8      4640799.0  Harry Potter and the Sorcerer's Stone (Harry P...\n",
      "9     13810543.0     Harry Potter: The Prequel (Harry Potter, #0.5)\n",
      "10     2556437.0  James Potter and the Hall of Elders' Crossing ...\n",
      "11    52218734.0  Short Stories from Hogwarts of Power, Politics...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0            Harry Potter and the Goblet of Fire (2005)\n",
       "1     Harry Potter and the Deathly Hallows: Part 2 (...\n",
       "2                               Maze Runner, The (2014)\n",
       "3     Harry Potter and the Deathly Hallows: Part 2 (...\n",
       "4     Harry Potter and the Deathly Hallows: Part 2 (...\n",
       "5                            Jellyfish (Meduzot) (2007)\n",
       "6            Harry Potter and the Goblet of Fire (2005)\n",
       "7            Harry Potter and the Goblet of Fire (2005)\n",
       "8     Harry Potter and the Deathly Hallows: Part 2 (...\n",
       "9            Harry Potter and the Goblet of Fire (2005)\n",
       "10                              Maze Runner, The (2014)\n",
       "11           Harry Potter and the Goblet of Fire (2005)\n",
       "Name: title_movie, dtype: object"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_local_reccs_movies(suggested_book_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputted books\n",
      "0              Harry Potter Boxset (Harry Potter, #1-7)\n",
      "1     Harry Potter and the Chamber of Secrets (Harry...\n",
      "2     Harry Potter and the Cursed Child - Parts One ...\n",
      "3     Harry Potter and the Deathly Hallows (Harry Po...\n",
      "4     Harry Potter and the Half-Blood Prince (Harry ...\n",
      "5           Harry Potter and the Methods of Rationality\n",
      "6     Harry Potter and the Order of the Phoenix (Har...\n",
      "7     Harry Potter and the Prisoner of Azkaban (Harr...\n",
      "8     Harry Potter and the Sorcerer's Stone (Harry P...\n",
      "9        Harry Potter: The Prequel (Harry Potter, #0.5)\n",
      "10    James Potter and the Hall of Elders' Crossing ...\n",
      "11    Short Stories from Hogwarts of Power, Politics...\n",
      "Name: title_book, dtype: object\n",
      "Top 5 movie recommendations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0           Harry Potter and the Goblet of Fire (2005)\n",
       "1    Harry Potter and the Deathly Hallows: Part 2 (...\n",
       "2                                      Twitches (2005)\n",
       "3                              Maze Runner, The (2014)\n",
       "4                     Pillars of the Earth, The (2010)\n",
       "Name: title_movie, dtype: object"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_global_reccs_movies(suggested_book_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
